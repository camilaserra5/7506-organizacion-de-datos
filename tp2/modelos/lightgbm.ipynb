{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from math import sqrt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\", low_memory=False)\n",
    "test_df = pd.read_csv(\"test.csv\", low_memory=False)\n",
    "data = pd.read_csv(\"data_set.csv\", low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500000.0     3041\n",
       "2500000.0     3036\n",
       "1200000.0     2909\n",
       "3500000.0     2726\n",
       "1300000.0     2480\n",
       "2200000.0     2420\n",
       "1100000.0     2394\n",
       "1800000.0     2290\n",
       "1600000.0     2217\n",
       "850000.0      2192\n",
       "750000.0      2163\n",
       "1250000.0     2159\n",
       "2300000.0     2108\n",
       "1350000.0     2103\n",
       "650000.0      2061\n",
       "3200000.0     2054\n",
       "4500000.0     2040\n",
       "2000000.0     1904\n",
       "950000.0      1867\n",
       "1650000.0     1865\n",
       "2800000.0     1863\n",
       "1400000.0     1850\n",
       "550000.0      1820\n",
       "2100000.0     1802\n",
       "3000000.0     1791\n",
       "1700000.0     1763\n",
       "1850000.0     1760\n",
       "2600000.0     1735\n",
       "1150000.0     1722\n",
       "1750000.0     1702\n",
       "              ... \n",
       "10807000.0       1\n",
       "468778.0         1\n",
       "675411.0         1\n",
       "468768.0         1\n",
       "7500250.0        1\n",
       "10815000.0       1\n",
       "3752490.0        1\n",
       "469105.0         1\n",
       "338144.0         1\n",
       "1352990.0        1\n",
       "938630.0         1\n",
       "10822700.0       1\n",
       "2705670.0        1\n",
       "1877000.0        1\n",
       "407500.0         1\n",
       "1352610.0        1\n",
       "676300.0         1\n",
       "2705184.0        1\n",
       "938426.0         1\n",
       "1876500.0        1\n",
       "1352560.0        1\n",
       "469208.0         1\n",
       "1352520.0        1\n",
       "469200.0         1\n",
       "5410000.0        1\n",
       "2704600.0        1\n",
       "1352250.0        1\n",
       "938260.0         1\n",
       "2704460.0        1\n",
       "2327000.0        1\n",
       "Name: precio, Length: 15520, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[['id','precio']]\n",
    "train_df['precio'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500000.0     3041\n",
       "2500000.0     3036\n",
       "1200000.0     2909\n",
       "3500000.0     2726\n",
       "1300000.0     2480\n",
       "2200000.0     2420\n",
       "1100000.0     2394\n",
       "1800000.0     2290\n",
       "1600000.0     2217\n",
       "850000.0      2192\n",
       "750000.0      2163\n",
       "1250000.0     2159\n",
       "2300000.0     2108\n",
       "1350000.0     2103\n",
       "650000.0      2061\n",
       "3200000.0     2054\n",
       "4500000.0     2040\n",
       "2000000.0     1904\n",
       "950000.0      1867\n",
       "1650000.0     1865\n",
       "2800000.0     1863\n",
       "1400000.0     1850\n",
       "550000.0      1820\n",
       "2100000.0     1802\n",
       "3000000.0     1791\n",
       "1700000.0     1763\n",
       "1850000.0     1760\n",
       "2600000.0     1735\n",
       "1150000.0     1722\n",
       "1750000.0     1702\n",
       "              ... \n",
       "10807000.0       1\n",
       "468778.0         1\n",
       "675411.0         1\n",
       "468768.0         1\n",
       "7500250.0        1\n",
       "10815000.0       1\n",
       "3752490.0        1\n",
       "469105.0         1\n",
       "338144.0         1\n",
       "1352990.0        1\n",
       "938630.0         1\n",
       "10822700.0       1\n",
       "2705670.0        1\n",
       "1877000.0        1\n",
       "407500.0         1\n",
       "1352610.0        1\n",
       "676300.0         1\n",
       "2705184.0        1\n",
       "938426.0         1\n",
       "1876500.0        1\n",
       "1352560.0        1\n",
       "469208.0         1\n",
       "1352520.0        1\n",
       "469200.0         1\n",
       "5410000.0        1\n",
       "2704600.0        1\n",
       "1352250.0        1\n",
       "938260.0         1\n",
       "2704460.0        1\n",
       "2327000.0        1\n",
       "Name: precio, Length: 15520, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = pd.merge(train_df, data, on='id', how='inner')\n",
    "features = features.fillna(0)\n",
    "features['precio'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  180000 Test:  60000\n"
     ]
    }
   ],
   "source": [
    "labels = features['precio']\n",
    "features = features.drop(['id','precio'], axis=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.25)\n",
    "print(\"Train: \",len(x_train),\"Test: \",len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'nthread':[4], \n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [.03, 0.05, .07], \n",
    "              'max_depth': [5, 6, 7],\n",
    "              'min_child_weight': [4],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(x_train, y_train)\n",
    "lgb_test = lgb.Dataset(x_test, y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = {}  # to record eval results for plotting\n",
    "lgb = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=500,\n",
    "                valid_sets=[lgb_train, lgb_test],\n",
    "                feature_name=['f' + str(i + 1) for i in range(x_train.shape[-1])],\n",
    "                categorical_feature=[21],\n",
    "                evals_result=evals_result,\n",
    "                verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_grid = GridSearchCV(gbm,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "estimator should be an estimator implementing 'fit' method, <lightgbm.basic.Booster object at 0x7fd1d817e5c0> was passed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-2cbb9aee14c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlgb_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         scorers, self.multimetric_ = _check_multimetric_scoring(\n\u001b[0;32m--> 608\u001b[0;31m             self.estimator, scoring=self.scoring)\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultimetric_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[0;34m(estimator, scoring)\u001b[0m\n\u001b[1;32m    340\u001b[0m     if callable(scoring) or scoring is None or isinstance(scoring,\n\u001b[1;32m    341\u001b[0m                                                           str):\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mscorers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"score\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscorers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36mcheck_scoring\u001b[0;34m(estimator, scoring, allow_none)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         raise TypeError(\"estimator should be an estimator implementing \"\n\u001b[0;32m--> 270\u001b[0;31m                         \"'fit' method, %r was passed\" % estimator)\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mget_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: estimator should be an estimator implementing 'fit' method, <lightgbm.basic.Booster object at 0x7fd1d817e5c0> was passed"
     ]
    }
   ],
   "source": [
    "lgb_grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
